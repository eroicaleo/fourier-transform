{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome! This is my notes to the exercises of Professor Brad Osgood's Lectures on the Fourier Transform and Its Applications. You can find this book from the publisher here .","title":"Home"},{"location":"#welcome","text":"This is my notes to the exercises of Professor Brad Osgood's Lectures on the Fourier Transform and Its Applications. You can find this book from the publisher here .","title":"Welcome!"},{"location":"appendix-b/","text":"Appendix B Complex Numbers and Complex Exponentials B.1. Complex Numbers i = \\sqrt[]{-1} By the way, read -i as \u201cminus i \u201d not \u201cnegative i .\u201d To say \u2212i is \u201cnegative i \u201d would imply that i is positive, and it isn\u2019t. This is because we can not have an order on \\mathbb{C} . The complex conjugate of z = x+iy is \\bar{z} = x - iy . We have \\overline{z + w} = \\overline{z} + \\overline{w}, \\overline{zw} = \\overline{z}\\overline{w}, \\overline{\\overline{z}} = z \\\\ x = \\text{Re} z = \\frac{z + \\overline{z}}{2} \\\\ y = \\text{Im} z = \\frac{z - \\overline{z}}{2i} \\\\ For the integral \\overline{ \\int_{a}^{b}f(t) g(t) dt } = \\int_{a}^{b} \\overline{f(t)} \\overline{g(t)} dt We can also right \\left( \\int_{a}^{b}f(t) g(t) dt \\right)^* = \\int_{a}^{b} f(t)^* g(t)^* dt The magnitude of z = x+iy is |z| = \\sqrt[]{x^2 + y^2} We also have |z|^2 = z \\overline{z} \\\\ |z+w|^2 =|z|^2 + 2 \\text{Re} \\{\\overline{z}w\\} + |w|^2 Polar form For z = x + yi , we can also write it as x + yi = r \\cos \u03b8+ i r \\sin \u03b8 = r (\\cos \u03b8+ i \\sin \u03b8) B.2. The Complex Exponential and Euler\u2019s Formula Define e^z = 1 + z + \\frac{z^2}{2!} + \\frac{z^3}{3!} + \\cdots = \\sum_{n = 0}^{\\infty} \\frac{z^n}{n!} This converges for all z \u2208 \\mathbb{C} , but we won\u2019t check that. Also we have \\overline{e^z} = e^{\\bar{z}} To prove e^{z+w} = e^z e^w Notice if f = e^z then f' = f, f(0) = 1 . And if we fix w , we have \\frac{d}{dz} e^{w+z} = e^{w+z} From the knowledge of difference equation, we must have e^{w+z} = c e^z Then plug z = 0 , we have c = e^w . Euler\u2019s formula f(\u03b8) = e^{i \u03b8} = \\cos \u03b8 + i \\sin \u03b8 = g(\u03b8) The left-hand side has only been defined via a series. The exponential function in the real case has nothing to do with the trig functions sine and cosine, and why it should have anything to do with them in the complex case is a true wonder. Use some results for di\ufb00erential equations (the results themselves are difficult). Note f(0) = 1, f'(\u03b8) = i e^{i\u03b8} , so f'(0) = i . Moreover f''(\u03b8) = - e^{i\u03b8} = -f(\u03b8) i.e. f satisfy f'' + f = 0, f(0) = 1, f'(0) = i On the other hand g''(\u03b8) = (-\\sin \u03b8 + i \\cos \u03b8)' = - (\\cos \u03b8 + i \\sin \u03b8) = - g(\u03b8) \\\\ So we have g'' + g = 0, g(0) = 1, g'(0) = i Thus f and g satisfy the same di\ufb00erential equation with the same initial conditions; so f and g must be equal. Slick. \\square Plugging \u03b8 = \u03c0 into Euler\u2019s formula, we have e^{i \\pi} + 1 = 0 This is sometimes referred to as the most famous equation in mathematics. It expresses a simple relationship \u2014 and why should there be any at all? \u2014 between the fundamental numbers e, \u03c0, 1, and 0 , not to mention i . We\u2019ll probably never see this most famous equation again, but now we\u2019ve seen it once. Consequences of Euler\u2019s formula. z = r (\\cos \u03b8 + i \\sin \u03b8) = r e^{i\u03b8} Then the multiplication of 2 complex numbers is z_1 z_2 = r_1 r_2 e^{i(\u03b8_1 + \u03b8_2)} We also have a dead easy way of deriving the addition formulas for the sine and cosine \\begin{split} e^{i(\u03b1+\u03b2)} &= e^{i\u03b1} e^{i\u03b2} \\\\ &= (\\cos \u03b1 + i \\sin \u03b1)(\\cos \u03b2 + i \\sin \u03b2) \\\\ &= (\\cos\u03b1\\cos\u03b2\u2212\\sin\u03b1\\sin\u03b2)+i(\\sin\u03b1\\cos\u03b2 +\\cos\u03b1\\sin\u03b2) \\end{split} On the other hand \\begin{split} e^{i(\u03b1+\u03b2)} = \\cos (\u03b1+\u03b2) + i \\sin (\u03b1+\u03b2) \\end{split} So we have \\cos(\u03b1+\u03b2) = \\cos\u03b1\\cos\u03b2\u2212\\sin\u03b1\\sin\u03b2, \\\\ \\sin(\u03b1+\u03b2) = \\sin\u03b1\\cos\u03b2 +\\cos\u03b1\\sin\u03b2. Quote from the author: I went through this derivation because it expresses in a simple way an extremely important principle in mathematics and its applications. If you can compute the same thing two di\ufb00erent ways, chances are you\u2019ve done something significant. Take this seriously. This maxim appears throughout the course. Symmetries of the sine and cosine: Even and odd functions. Use this \\overline{e^{i\u03b8}} = e^{\\overline{i\u03b8}} = e^{-i\u03b8} We have \\cos \u03b8 = \\frac{e^{i\u03b8} + e^{-i\u03b8}}{2}, i\\sin \u03b8 = \\frac{e^{i\u03b8} - e^{-i\u03b8}}{2}. Note \\cos is an even function and \\sin is an odd function. Why is this true? There are many ways of seeing it (Taylor series, di\ufb00erential equations), but here\u2019s one you may not have thought of before, and it fits into a general framework of evenness and oddness that we\u2019ll find useful when discussing symmetries of the Fourier transform. Given any function f(x) , f_e = \\frac{f(x) + f(-x)}{2} \\\\ f_o = \\frac{f(x) - f(-x)}{2} The top one is an even function and the bottom one is an odd function. Algebra and geometry. |e^{i\u03b8}| = \\sqrt[]{ \\cos^2 \u03b8 + \\sin^2 \u03b8 } = 1 B.3. Further Applications of Euler\u2019s Formula For additional applications we\u2019ll replace \u03b8 by t and think of e^{it} = \\cos t + i \\sin t we see that e^{it} is a (complex-valued) parametrization of the circle: the circle is traced out exactly once in the counterclockwise direction as t goes from 0 to 2\u03c0 . For our e\ufb00orts we prefer to work with e^{i2\u03c0t} = \\cos (2\u03c0t) + i \\sin (2\u03c0t) the complex exponential e^{2\u03c0it} contains the sinusoids \\cos 2\u03c0t and \\sin 2\u03c0t , each of frequency 1 Hz. With the 2\u03c0 , as t goes from 0 to 1 the point e^{2\u03c0it} traces out the unit circle exactly once (one cycle) in a counter clockwise direction. The units in the exponential e^{2\u03c0it} are (as they are in \\cos 2\u03c0t and \\sin 2\u03c0t ) e^{ (2\u03c0 \\text{ radians/cycle}) \\cdot i \\cdot 1 (\\text{ cycle/sec}) \\cdot (t \\text{ sec}) } Without the 2\u03c0 the units in e^{it} are e^{ i \\cdot (1 \\text{ radians/cycle}) \\cdot (t \\text{ sec}) } change the amplitude and frequency and to include a phase shift The general complex exponential that includes this information is then A e^{i(2\u03c0\u03bdt + \\phi)} Then the point traces out the circle in the counterclockwise direction at a rate of \u03bd cycles per second; The phase \\phi determines the starting point on the circle, for at t = 0 the point is A e^{i\\phi} In fact, we can write A e^{i(2\u03c0\u03bdt + \\phi)} = e^{2\u03c0i\u03bdt} A e^{i\\phi} We can think of this as the (initial) vector Ae^{i\\phi} set rotating at a frequency \u03bd Hz through multiplication by the time-varying phasor e^{2\u03c0i\u03bdt} . What happens when \u03bd is negative? That simply reverses the direction of motion around the circle from counterclockwise to clockwise. The catch phrase is just so: * positive frequency means counterclockwise rotation * negative frequency means clockwise rotation. Sums of sinusoids. What one can say about the superposition of two signals? A_1 \\sin (2\u03c0\u03bdt + \\phi_1) + A_2 \\sin (2\u03c0\u03bdt + \\phi_2) Consider A_1 e^{i(2\u03c0\u03bdt + \\phi_1)} + A_2 e^{i(2\u03c0\u03bdt + \\phi_2)} whose imaginary part is the sum of sinusoids, above. Before messing with the algebra, think geometrically in terms of rotating vectors. At t = 0 we have the two vectors from the origin to the starting points z_0 = A_1 e^{i\u03c6_1}, w_0 = A_2 e^{i\u03c6_2}. Their sum z_0 +w_0 is the starting point (or starting vector) for the sum of the two motions. But how do those two starting vectors move? They rotate together at the same rate, the motion of each described by e^{2\u03c0i\u03bdt}z_0 and e^{2\u03c0i\u03bdt}w_0 , respectively. Thus their sum also rotates at that rate \u2014 think of the whole parallelogram (vector sum) rotating rigidly about the vertex at the origin. Now mess with the algebra and arrive at the same result: A_1 e^{i(2\u03c0\u03bdt + \\phi_1)} + A_2 e^{i(2\u03c0\u03bdt + \\phi_2)} = e^{i2\u03c0\u03bdt} ( A_1 e^{i\\phi_1} + A_2 e^{i\\phi_2} ) Problems and Further Results B.1. If 0 \u2264 \\arg w \u2212 \\arg z < \u03c0 , show that the area of the triangle whose vertices are 0 , z , and w is given by \\frac{1}{2} \\Im \\{\\overline{z}w\\} Proof : Assume z, w is shown in the figure below. w = r_1 e^{i\u03b8_1}, z = r_2 e^{i\u03b8_2} Then if we rotate the triangle until z is completely on x -axis, then we can see the area is \\frac{1}{2} r_2 \\cdot r_1 \\cdot \\sin (\u03b8_1 - \u03b8_2) Also note \\overline{z}w\\ = r_2 e^{-i\u03b8_2} r_1 e^{i\u03b8_1} \\\\ = r_2 r_1 e^{i(\u03b8_1-\u03b8_2)} \\\\ = r_2 r_1 (\\cos (\u03b8_1-\u03b8_2) + i \\sin (\u03b8_1-\u03b8_2)) So we have A = \\frac{1}{2}\\Im \\{\\overline{z}w\\} . \\square B.2. Show that the equation of a line can be written as az + \\overline{a}\\overline{z}+b = 0 , where a is a complex number and b is real (and z = x+iy is the variable). What is the slope of the line? Solution : Let a = r_a e^{i\u03b8_a}, z = r_z e^{i\u03b8_z} . Then -b = az + \\overline{a}\\overline{z} = 2 \\Re\\{az\\} = 2 r_a r_z \\cos (\u03b8_a + \u03b8_z) \\\\ \\Rightarrow \\\\ r_z \\cos (\u03b8_a + \u03b8_z) = -\\frac{b}{2 r_a} I think the problem is asking, given a line like in the figure below, how to find complex number a and real number b , such that as long as z satisfy az + \\overline{a}\\overline{z}+b = 0 , then z will be on that line. Assume the distance between 0 and the line is d , and the angle between x-axis and the dash line is \u03b8_a . From the figure, we can see, if we rotate z clockwise by \u03b8_a , to get z' , then z' is on the vertical line. So we can have a = e^{-i\u03b8_a}, b = -2d The if z satisfy az + \\overline{a}\\overline{z}+b = 0 , z will be on the line. Then slop of the line is \\tan (\\frac{\\pi }{2} - \u03b8_a) . B.3. Recall the identity |z +w|^2 = |z|^2 +|w|^2 +2\\Re\\{\\overline{z} w\\} . As a generalization of this, show that |z +w|^2 +|z + \\overline{w}|^2 = 2(|z|^2 +|w|^2) + 4 \\Re \\{z\\} \\Re \\{w\\} \\\\ |z +w|^2 +|z - \\overline{w}|^2 = 2(|z|^2 +|w|^2) + 4 \\Im \\{z\\} \\Im \\{w\\} \\\\ Proof : For the first part 2\\Re\\{\\overline{z} w\\} + 2\\Re\\{\\overline{z} \\overline{w}\\} \\\\ = 2 \\Re \\{\\overline{z} w +\\overline{z} \\overline{w} \\} \\\\ = 2 \\Re \\{\\overline{z} (w + \\overline{w}) \\} \\\\ = 2 \\Re \\{\\overline{z} 2 \\Re \\{w\\} \\} \\\\ = 4 \\Re \\{w\\} \\Re \\{\\overline{z}\\} \\\\ = 4 \\Re \\{w\\} \\Re \\{z\\} \\\\ For the second part 2\\Re\\{\\overline{z} w\\} + 2\\Re\\{\\overline{z} (-\\overline{w})\\} \\\\ = 2 \\Re \\{\\overline{z} w +\\overline{z} (-\\overline{w}) \\} \\\\ = 2 \\Re \\{\\overline{z} (w - \\overline{w}) \\} \\\\ = 2 \\Re \\{\\overline{z} 2 \\Im \\{w\\} i \\} \\\\ = 4 \\Im \\{w\\} \\Re \\{\\overline{z} i\\} \\\\ = 4 \\Im \\{w\\} \\Im \\{z\\} \\\\ \\square","title":"Appendix B"},{"location":"appendix-b/#appendix-b-complex-numbers-and-complex-exponentials","text":"","title":"Appendix B Complex Numbers and Complex Exponentials"},{"location":"appendix-b/#b1-complex-numbers","text":"i = \\sqrt[]{-1} By the way, read -i as \u201cminus i \u201d not \u201cnegative i .\u201d To say \u2212i is \u201cnegative i \u201d would imply that i is positive, and it isn\u2019t. This is because we can not have an order on \\mathbb{C} . The complex conjugate of z = x+iy is \\bar{z} = x - iy . We have \\overline{z + w} = \\overline{z} + \\overline{w}, \\overline{zw} = \\overline{z}\\overline{w}, \\overline{\\overline{z}} = z \\\\ x = \\text{Re} z = \\frac{z + \\overline{z}}{2} \\\\ y = \\text{Im} z = \\frac{z - \\overline{z}}{2i} \\\\ For the integral \\overline{ \\int_{a}^{b}f(t) g(t) dt } = \\int_{a}^{b} \\overline{f(t)} \\overline{g(t)} dt We can also right \\left( \\int_{a}^{b}f(t) g(t) dt \\right)^* = \\int_{a}^{b} f(t)^* g(t)^* dt The magnitude of z = x+iy is |z| = \\sqrt[]{x^2 + y^2} We also have |z|^2 = z \\overline{z} \\\\ |z+w|^2 =|z|^2 + 2 \\text{Re} \\{\\overline{z}w\\} + |w|^2 Polar form For z = x + yi , we can also write it as x + yi = r \\cos \u03b8+ i r \\sin \u03b8 = r (\\cos \u03b8+ i \\sin \u03b8)","title":"B.1. Complex Numbers"},{"location":"appendix-b/#b2-the-complex-exponential-and-eulers-formula","text":"Define e^z = 1 + z + \\frac{z^2}{2!} + \\frac{z^3}{3!} + \\cdots = \\sum_{n = 0}^{\\infty} \\frac{z^n}{n!} This converges for all z \u2208 \\mathbb{C} , but we won\u2019t check that. Also we have \\overline{e^z} = e^{\\bar{z}} To prove e^{z+w} = e^z e^w Notice if f = e^z then f' = f, f(0) = 1 . And if we fix w , we have \\frac{d}{dz} e^{w+z} = e^{w+z} From the knowledge of difference equation, we must have e^{w+z} = c e^z Then plug z = 0 , we have c = e^w .","title":"B.2. The Complex Exponential and Euler\u2019s Formula"},{"location":"appendix-b/#eulers-formula","text":"f(\u03b8) = e^{i \u03b8} = \\cos \u03b8 + i \\sin \u03b8 = g(\u03b8) The left-hand side has only been defined via a series. The exponential function in the real case has nothing to do with the trig functions sine and cosine, and why it should have anything to do with them in the complex case is a true wonder. Use some results for di\ufb00erential equations (the results themselves are difficult). Note f(0) = 1, f'(\u03b8) = i e^{i\u03b8} , so f'(0) = i . Moreover f''(\u03b8) = - e^{i\u03b8} = -f(\u03b8) i.e. f satisfy f'' + f = 0, f(0) = 1, f'(0) = i On the other hand g''(\u03b8) = (-\\sin \u03b8 + i \\cos \u03b8)' = - (\\cos \u03b8 + i \\sin \u03b8) = - g(\u03b8) \\\\ So we have g'' + g = 0, g(0) = 1, g'(0) = i Thus f and g satisfy the same di\ufb00erential equation with the same initial conditions; so f and g must be equal. Slick. \\square Plugging \u03b8 = \u03c0 into Euler\u2019s formula, we have e^{i \\pi} + 1 = 0 This is sometimes referred to as the most famous equation in mathematics. It expresses a simple relationship \u2014 and why should there be any at all? \u2014 between the fundamental numbers e, \u03c0, 1, and 0 , not to mention i . We\u2019ll probably never see this most famous equation again, but now we\u2019ve seen it once.","title":"Euler\u2019s formula"},{"location":"appendix-b/#consequences-of-eulers-formula","text":"z = r (\\cos \u03b8 + i \\sin \u03b8) = r e^{i\u03b8} Then the multiplication of 2 complex numbers is z_1 z_2 = r_1 r_2 e^{i(\u03b8_1 + \u03b8_2)} We also have a dead easy way of deriving the addition formulas for the sine and cosine \\begin{split} e^{i(\u03b1+\u03b2)} &= e^{i\u03b1} e^{i\u03b2} \\\\ &= (\\cos \u03b1 + i \\sin \u03b1)(\\cos \u03b2 + i \\sin \u03b2) \\\\ &= (\\cos\u03b1\\cos\u03b2\u2212\\sin\u03b1\\sin\u03b2)+i(\\sin\u03b1\\cos\u03b2 +\\cos\u03b1\\sin\u03b2) \\end{split} On the other hand \\begin{split} e^{i(\u03b1+\u03b2)} = \\cos (\u03b1+\u03b2) + i \\sin (\u03b1+\u03b2) \\end{split} So we have \\cos(\u03b1+\u03b2) = \\cos\u03b1\\cos\u03b2\u2212\\sin\u03b1\\sin\u03b2, \\\\ \\sin(\u03b1+\u03b2) = \\sin\u03b1\\cos\u03b2 +\\cos\u03b1\\sin\u03b2. Quote from the author: I went through this derivation because it expresses in a simple way an extremely important principle in mathematics and its applications. If you can compute the same thing two di\ufb00erent ways, chances are you\u2019ve done something significant. Take this seriously. This maxim appears throughout the course.","title":"Consequences of Euler\u2019s formula."},{"location":"appendix-b/#symmetries-of-the-sine-and-cosine-even-and-odd-functions","text":"Use this \\overline{e^{i\u03b8}} = e^{\\overline{i\u03b8}} = e^{-i\u03b8} We have \\cos \u03b8 = \\frac{e^{i\u03b8} + e^{-i\u03b8}}{2}, i\\sin \u03b8 = \\frac{e^{i\u03b8} - e^{-i\u03b8}}{2}. Note \\cos is an even function and \\sin is an odd function. Why is this true? There are many ways of seeing it (Taylor series, di\ufb00erential equations), but here\u2019s one you may not have thought of before, and it fits into a general framework of evenness and oddness that we\u2019ll find useful when discussing symmetries of the Fourier transform. Given any function f(x) , f_e = \\frac{f(x) + f(-x)}{2} \\\\ f_o = \\frac{f(x) - f(-x)}{2} The top one is an even function and the bottom one is an odd function.","title":"Symmetries of the sine and cosine: Even and odd functions."},{"location":"appendix-b/#algebra-and-geometry","text":"|e^{i\u03b8}| = \\sqrt[]{ \\cos^2 \u03b8 + \\sin^2 \u03b8 } = 1","title":"Algebra and geometry."},{"location":"appendix-b/#b3-further-applications-of-eulers-formula","text":"For additional applications we\u2019ll replace \u03b8 by t and think of e^{it} = \\cos t + i \\sin t we see that e^{it} is a (complex-valued) parametrization of the circle: the circle is traced out exactly once in the counterclockwise direction as t goes from 0 to 2\u03c0 . For our e\ufb00orts we prefer to work with e^{i2\u03c0t} = \\cos (2\u03c0t) + i \\sin (2\u03c0t) the complex exponential e^{2\u03c0it} contains the sinusoids \\cos 2\u03c0t and \\sin 2\u03c0t , each of frequency 1 Hz. With the 2\u03c0 , as t goes from 0 to 1 the point e^{2\u03c0it} traces out the unit circle exactly once (one cycle) in a counter clockwise direction. The units in the exponential e^{2\u03c0it} are (as they are in \\cos 2\u03c0t and \\sin 2\u03c0t ) e^{ (2\u03c0 \\text{ radians/cycle}) \\cdot i \\cdot 1 (\\text{ cycle/sec}) \\cdot (t \\text{ sec}) } Without the 2\u03c0 the units in e^{it} are e^{ i \\cdot (1 \\text{ radians/cycle}) \\cdot (t \\text{ sec}) }","title":"B.3. Further Applications of Euler\u2019s Formula"},{"location":"appendix-b/#change-the-amplitude-and-frequency-and-to-include-a-phase-shift","text":"The general complex exponential that includes this information is then A e^{i(2\u03c0\u03bdt + \\phi)} Then the point traces out the circle in the counterclockwise direction at a rate of \u03bd cycles per second; The phase \\phi determines the starting point on the circle, for at t = 0 the point is A e^{i\\phi} In fact, we can write A e^{i(2\u03c0\u03bdt + \\phi)} = e^{2\u03c0i\u03bdt} A e^{i\\phi} We can think of this as the (initial) vector Ae^{i\\phi} set rotating at a frequency \u03bd Hz through multiplication by the time-varying phasor e^{2\u03c0i\u03bdt} . What happens when \u03bd is negative? That simply reverses the direction of motion around the circle from counterclockwise to clockwise. The catch phrase is just so: * positive frequency means counterclockwise rotation * negative frequency means clockwise rotation.","title":"change the amplitude and frequency and to include a phase shift"},{"location":"appendix-b/#sums-of-sinusoids","text":"What one can say about the superposition of two signals? A_1 \\sin (2\u03c0\u03bdt + \\phi_1) + A_2 \\sin (2\u03c0\u03bdt + \\phi_2) Consider A_1 e^{i(2\u03c0\u03bdt + \\phi_1)} + A_2 e^{i(2\u03c0\u03bdt + \\phi_2)} whose imaginary part is the sum of sinusoids, above. Before messing with the algebra, think geometrically in terms of rotating vectors. At t = 0 we have the two vectors from the origin to the starting points z_0 = A_1 e^{i\u03c6_1}, w_0 = A_2 e^{i\u03c6_2}. Their sum z_0 +w_0 is the starting point (or starting vector) for the sum of the two motions. But how do those two starting vectors move? They rotate together at the same rate, the motion of each described by e^{2\u03c0i\u03bdt}z_0 and e^{2\u03c0i\u03bdt}w_0 , respectively. Thus their sum also rotates at that rate \u2014 think of the whole parallelogram (vector sum) rotating rigidly about the vertex at the origin. Now mess with the algebra and arrive at the same result: A_1 e^{i(2\u03c0\u03bdt + \\phi_1)} + A_2 e^{i(2\u03c0\u03bdt + \\phi_2)} = e^{i2\u03c0\u03bdt} ( A_1 e^{i\\phi_1} + A_2 e^{i\\phi_2} )","title":"Sums of sinusoids."},{"location":"appendix-b/#problems-and-further-results","text":"","title":"Problems and Further Results"},{"location":"appendix-b/#b1","text":"If 0 \u2264 \\arg w \u2212 \\arg z < \u03c0 , show that the area of the triangle whose vertices are 0 , z , and w is given by \\frac{1}{2} \\Im \\{\\overline{z}w\\} Proof : Assume z, w is shown in the figure below. w = r_1 e^{i\u03b8_1}, z = r_2 e^{i\u03b8_2} Then if we rotate the triangle until z is completely on x -axis, then we can see the area is \\frac{1}{2} r_2 \\cdot r_1 \\cdot \\sin (\u03b8_1 - \u03b8_2) Also note \\overline{z}w\\ = r_2 e^{-i\u03b8_2} r_1 e^{i\u03b8_1} \\\\ = r_2 r_1 e^{i(\u03b8_1-\u03b8_2)} \\\\ = r_2 r_1 (\\cos (\u03b8_1-\u03b8_2) + i \\sin (\u03b8_1-\u03b8_2)) So we have A = \\frac{1}{2}\\Im \\{\\overline{z}w\\} . \\square","title":"B.1."},{"location":"appendix-b/#b2","text":"Show that the equation of a line can be written as az + \\overline{a}\\overline{z}+b = 0 , where a is a complex number and b is real (and z = x+iy is the variable). What is the slope of the line? Solution : Let a = r_a e^{i\u03b8_a}, z = r_z e^{i\u03b8_z} . Then -b = az + \\overline{a}\\overline{z} = 2 \\Re\\{az\\} = 2 r_a r_z \\cos (\u03b8_a + \u03b8_z) \\\\ \\Rightarrow \\\\ r_z \\cos (\u03b8_a + \u03b8_z) = -\\frac{b}{2 r_a} I think the problem is asking, given a line like in the figure below, how to find complex number a and real number b , such that as long as z satisfy az + \\overline{a}\\overline{z}+b = 0 , then z will be on that line. Assume the distance between 0 and the line is d , and the angle between x-axis and the dash line is \u03b8_a . From the figure, we can see, if we rotate z clockwise by \u03b8_a , to get z' , then z' is on the vertical line. So we can have a = e^{-i\u03b8_a}, b = -2d The if z satisfy az + \\overline{a}\\overline{z}+b = 0 , z will be on the line. Then slop of the line is \\tan (\\frac{\\pi }{2} - \u03b8_a) .","title":"B.2."},{"location":"appendix-b/#b3","text":"Recall the identity |z +w|^2 = |z|^2 +|w|^2 +2\\Re\\{\\overline{z} w\\} . As a generalization of this, show that |z +w|^2 +|z + \\overline{w}|^2 = 2(|z|^2 +|w|^2) + 4 \\Re \\{z\\} \\Re \\{w\\} \\\\ |z +w|^2 +|z - \\overline{w}|^2 = 2(|z|^2 +|w|^2) + 4 \\Im \\{z\\} \\Im \\{w\\} \\\\ Proof : For the first part 2\\Re\\{\\overline{z} w\\} + 2\\Re\\{\\overline{z} \\overline{w}\\} \\\\ = 2 \\Re \\{\\overline{z} w +\\overline{z} \\overline{w} \\} \\\\ = 2 \\Re \\{\\overline{z} (w + \\overline{w}) \\} \\\\ = 2 \\Re \\{\\overline{z} 2 \\Re \\{w\\} \\} \\\\ = 4 \\Re \\{w\\} \\Re \\{\\overline{z}\\} \\\\ = 4 \\Re \\{w\\} \\Re \\{z\\} \\\\ For the second part 2\\Re\\{\\overline{z} w\\} + 2\\Re\\{\\overline{z} (-\\overline{w})\\} \\\\ = 2 \\Re \\{\\overline{z} w +\\overline{z} (-\\overline{w}) \\} \\\\ = 2 \\Re \\{\\overline{z} (w - \\overline{w}) \\} \\\\ = 2 \\Re \\{\\overline{z} 2 \\Im \\{w\\} i \\} \\\\ = 4 \\Im \\{w\\} \\Re \\{\\overline{z} i\\} \\\\ = 4 \\Im \\{w\\} \\Im \\{z\\} \\\\ \\square","title":"B.3."},{"location":"ch01ex/","text":"1. Fourier Series 1.1. Playing with Fourier series using MATLAB Solution : Since I do not have the matlab code yet, I use python to draw the following: import numpy as np import matplotlib.pyplot as plt from matplotlib.ticker import MultipleLocator # ex 1.1 # Generate x values from 0 to 10 with a step of 0.1 t = np.arange(0, 2, 0.01) # Calculate the amplitude as the sine of the time values f = np.zeros_like(t) for n in range(5): amp = -15.0 / (1+n); phase = -0.1 * n f += amp * np.sin(2*np.pi*n*t + phase) # Plot the sine wave plt.plot(t, f) # Add labels and title plt.title('Sine wave') plt.xlabel('Time') plt.ylabel('Amplitude = sin(time)') plt.gca().xaxis.set_major_locator(MultipleLocator(0.2)) # Add a grid and a horizontal line at y=0 plt.grid(True, which='both') plt.axhline(y=0, color='k') # Display the plot plt.show() 1.2. Adding periodic functions You can sometimes be misled by thinking too casually about modifying or combining periodic functions: scaling a periodic function results in a periodic function; shifting a periodic function results in a periodic function. What about adding? (a) Let f(x) = \\sin(2\u03c0mx) + \\sin(2\u03c0nx) where n and m are positive integers. Is f(x) periodic? If so, what is its period? Solution : Yes, because apprantly f(x+1) = f(x) . If d = \\gcd(m, n) , then T = \\frac{1}{d} . I cannot prove it though. (b) Let g(x) = \\sin(2\u03c0px) + \\sin(2\u03c0qx) where p and q are positive rational numbers (say p= m/r and q= n/s , as fractions in lowest terms). Is g(x) periodic? If so, what is its period? Solution : Yes, the period is \\frac{\\text{lcm}(r,s)}{\\gcd(m,n)} \\square (c) It\u2019s not true that the sum of two periodic functions is periodic. For example, show that f(t) = \\cos t+\\cos \\sqrt[]{2} t is not periodic. (Hint: Suppose by way of contradiction that there is some T such that f(t+T) = f(t) for all t . In particular, the maximum value of f(t) repeats. This will lead to a contradiction.) Proof : Based on the hint, note that when t=0 , f(t) reaches its maximum, i.e. 2 . Note assume f(t+T) = f(t) , especially f(T) = f(0) = 2 . T = 2 n \\pi \\\\ \\sqrt[]{2} T = 2 m \\pi \\\\ Then we have \\sqrt[]{2} = \\frac{m}{n} This is not possible. So f(t) is not a periodic function. 1.3 Periods of sums and products Let f(t) = \\sin3t+\\cos5t and g(t) = \\sin3t\u00b7\\cos5t (a) What is the period of f(t) ? Find the Fourier series for f(t) . Solution : The period of \\sin 3t is 2/3 \\pi , and the period of \\cos5t is 2/5 \\pi . Since 3, 5 are coprime, then their common period has to be 2 \\pi . \\begin{split} c_n &= \\frac{1}{2\u03c0} \\int_{0}^{2 \\pi } \\sin 3t \u00b7 e^{-2\u03c0int / (2\u03c0)} \\\\ &= \\frac{1}{2\u03c0} \\int_{0}^{2 \\pi } \\sin 3t \u00b7 e^{-int} \\\\ &= \\frac{1}{2\u03c0} \\int_{-\\pi}^{\\pi} \\sin 3t \u00b7 (\\cos (-nt) + i \\sin (-nt)) \\\\ &= \\frac{1}{2\u03c0} \\int_{-\\pi}^{\\pi} \\sin 3t \u00b7 (\\cos (nt) - i \\sin (nt)) \\\\ \\end{split} We use the conclusion from Understanding Analysis Exercise 8.5.2. c_n = \\begin{cases} -i/2 &\\text{if } n = 3 \\\\ i/2 &\\text{if } n = -3 \\\\ 0 &\\text{else}\\\\ \\end{cases} So \\sin 3t = \\frac{i}{2} e^{-3it} - \\frac{i}{2} e^{3it} On the hindsight, we can just use the complex number knowledge in Appendix B to get this. For the same reason \\cos 5t = \\frac{1}{2} e^{5it} - \\frac{1}{2} e^{-5it} So \\sin 3t + \\cos 5t = - \\frac{1}{2} e^{-5it} + \\frac{i}{2} e^{-3it} - \\frac{i}{2} e^{3it} + \\frac{1}{2} e^{5it} \\square (b) Find the Fourier series for g(t) . What is the period of g(t) ? (The period of the product is more interesting. The product repeats every 2\u03c0 , that is, g(t+2\u03c0) = g(t) , so the period of g(t) is a divisor of 2\u03c0 . To determine the fundamental frequency of g(t) , we find its Fourier series.) Solution : \\begin{split} \\sin3t\u00b7\\cos5t &= \\frac{e^{3it} - e^{-3it}}{2i} \\cdot \\frac{e^{5it} + e^{-5it}}{2} \\\\ &=\\frac{1}{4i} (-e^{-8it} + e^{-2it} - e^{2it} + e^{8it}) \\end{split} From this Fourier series, we can see, \u03c0 is a period. \\sin 3(t+\u03c0) \\cdot \\cos 5(t+\u03c0) \\\\ = (- \\sin 3t) \\cdot (-\\cos 5(t)) \\\\ = \\sin3t\u00b7\\cos5t \\square 1.4 Di\ufb00erent definitions of periodicity (a) Show that f(t) is periodic of period p if and only if f(t\u2212p) = f(t) for all t . The upshot is that it doesn\u2019t matter if we define periodicity as f(t+p) = f(t) or as f(t\u2212p) = f(t) . Proof : If f(t) is periodic, then f(t+p) = f(t) , let s = t + p , then t = s - p . So we have f(s\u2212p) = f(s), \\forall s The other side is the same. \\square (b) Show that f(t) is periodic of period p if and only if f(t + p/2) = f(t\u2212p/2) for all t . Proof : If f(t) is periodic, then f(t+p) = f(t) , let s = t - p/2 , then t = s + p/2 . So we have f(s + p/2) = f(s\u2212p/2), \\forall s The other side is the same. \\square 1.5. Overheard at a problem session ... Suppose two sinusoids have the same frequency but possibly di\ufb00erent amplitudes and phases. What about their sum? Each of the following answers was proposed at a problem session: (a) Has twice the frequency of the original sinusoids. (b) Is either zero or has exactly the same frequency as the original sinusoids. (c) May exhibit beats. (d) Is not necessarily periodic at all. Which is correct, and why? (Hint: Think in terms of complex exponentials.) Solution : This is covered at the end of Appendix B. \\square 1.6. Low voltage A periodic voltage is given by v(t) = 3 \\cos (2\u03c0\u03bd_1t\u22121.3)+5 \\cos (2\u03c0\u03bd_2t+0.5). Regardless of the frequencies \u03bd_1 , \u03bd_2 , the maximum voltage is always less than 8 , but it can be much smaller. Use MATLAB (or another program) to find the maximum voltage if \u03bd_1 = 2 Hz and \u03bd_2 = 1 Hz. Solution : Here is the python code t = np.arange(0, 2, 0.01) f = 3 * np.cos(2 * np.pi * 2 * t - 1.3) + 5 * np.cos(2 * np.pi * 1 * t + 0.5) plt.plot(t, f) plt.show() print(max(np.around(f, 4)), min(np.around(f, 4))) 5.7807 -7.6865 1.7. Periodizing a triangle The triangle function with a parameter p > 0 is \\Lambda_p(t) = \\begin{cases} 1 - \\frac{1}{p}|t|, &|t| \\leq p, \\\\ 0, &|t| \\geq p. \\\\ \\end{cases} We\u2019ll be seeing it a lot. When p = 1 (the model case in many instances), we simplify the notation to just \u039b(t) . Note that \u039b_p(t) = \u039b(t/p) . The parameter p specifies the length of the base, namely 2p . Alternately, p determines the slopes of the sides: the left side has slope 1/p and the right side has slope \u22121/p . Now for T > 0 define g(t) = \\sum_{n = -\\infty}^{\\infty} \u039b_p(t - nT) Anwser: (a) For p = 1/2 and T = 1/2, T = 3/4, T = 1, T = 2 , sketch the graphs of g(t) . Periodic? In each case what is the period? Solution : The period are 0, 6/8, 1, 2 respectively. (b) In which of these cases is it possible to recover the original signal \u039b_{1/2}(t) from the sum g(t) ? What operation would you apply to g(t) to do this? Solution : When T = 1, 2 . We can times g(t) with the following h(t) h(t) = \\begin{cases} 1 &\\text{if } -1/2 \\leq t \\leq 1/2 \\\\ 0 &\\text{otherwise } \\\\ \\end{cases} \\square (c) In general, what condition on p and T will guarantee that \u039b_p(t) can be recovered from the sum g(t) ? Solution : In general, we need T \\geq 2p . \\square","title":"Chapter 01 Exercises"},{"location":"ch01ex/#1-fourier-series","text":"","title":"1. Fourier Series"},{"location":"ch01ex/#11-playing-with-fourier-series-using-matlab","text":"Solution : Since I do not have the matlab code yet, I use python to draw the following: import numpy as np import matplotlib.pyplot as plt from matplotlib.ticker import MultipleLocator # ex 1.1 # Generate x values from 0 to 10 with a step of 0.1 t = np.arange(0, 2, 0.01) # Calculate the amplitude as the sine of the time values f = np.zeros_like(t) for n in range(5): amp = -15.0 / (1+n); phase = -0.1 * n f += amp * np.sin(2*np.pi*n*t + phase) # Plot the sine wave plt.plot(t, f) # Add labels and title plt.title('Sine wave') plt.xlabel('Time') plt.ylabel('Amplitude = sin(time)') plt.gca().xaxis.set_major_locator(MultipleLocator(0.2)) # Add a grid and a horizontal line at y=0 plt.grid(True, which='both') plt.axhline(y=0, color='k') # Display the plot plt.show()","title":"1.1. Playing with Fourier series using MATLAB"},{"location":"ch01ex/#12-adding-periodic-functions","text":"You can sometimes be misled by thinking too casually about modifying or combining periodic functions: scaling a periodic function results in a periodic function; shifting a periodic function results in a periodic function. What about adding? (a) Let f(x) = \\sin(2\u03c0mx) + \\sin(2\u03c0nx) where n and m are positive integers. Is f(x) periodic? If so, what is its period? Solution : Yes, because apprantly f(x+1) = f(x) . If d = \\gcd(m, n) , then T = \\frac{1}{d} . I cannot prove it though. (b) Let g(x) = \\sin(2\u03c0px) + \\sin(2\u03c0qx) where p and q are positive rational numbers (say p= m/r and q= n/s , as fractions in lowest terms). Is g(x) periodic? If so, what is its period? Solution : Yes, the period is \\frac{\\text{lcm}(r,s)}{\\gcd(m,n)} \\square (c) It\u2019s not true that the sum of two periodic functions is periodic. For example, show that f(t) = \\cos t+\\cos \\sqrt[]{2} t is not periodic. (Hint: Suppose by way of contradiction that there is some T such that f(t+T) = f(t) for all t . In particular, the maximum value of f(t) repeats. This will lead to a contradiction.) Proof : Based on the hint, note that when t=0 , f(t) reaches its maximum, i.e. 2 . Note assume f(t+T) = f(t) , especially f(T) = f(0) = 2 . T = 2 n \\pi \\\\ \\sqrt[]{2} T = 2 m \\pi \\\\ Then we have \\sqrt[]{2} = \\frac{m}{n} This is not possible. So f(t) is not a periodic function.","title":"1.2. Adding periodic functions"},{"location":"ch01ex/#13-periods-of-sums-and-products","text":"Let f(t) = \\sin3t+\\cos5t and g(t) = \\sin3t\u00b7\\cos5t (a) What is the period of f(t) ? Find the Fourier series for f(t) . Solution : The period of \\sin 3t is 2/3 \\pi , and the period of \\cos5t is 2/5 \\pi . Since 3, 5 are coprime, then their common period has to be 2 \\pi . \\begin{split} c_n &= \\frac{1}{2\u03c0} \\int_{0}^{2 \\pi } \\sin 3t \u00b7 e^{-2\u03c0int / (2\u03c0)} \\\\ &= \\frac{1}{2\u03c0} \\int_{0}^{2 \\pi } \\sin 3t \u00b7 e^{-int} \\\\ &= \\frac{1}{2\u03c0} \\int_{-\\pi}^{\\pi} \\sin 3t \u00b7 (\\cos (-nt) + i \\sin (-nt)) \\\\ &= \\frac{1}{2\u03c0} \\int_{-\\pi}^{\\pi} \\sin 3t \u00b7 (\\cos (nt) - i \\sin (nt)) \\\\ \\end{split} We use the conclusion from Understanding Analysis Exercise 8.5.2. c_n = \\begin{cases} -i/2 &\\text{if } n = 3 \\\\ i/2 &\\text{if } n = -3 \\\\ 0 &\\text{else}\\\\ \\end{cases} So \\sin 3t = \\frac{i}{2} e^{-3it} - \\frac{i}{2} e^{3it} On the hindsight, we can just use the complex number knowledge in Appendix B to get this. For the same reason \\cos 5t = \\frac{1}{2} e^{5it} - \\frac{1}{2} e^{-5it} So \\sin 3t + \\cos 5t = - \\frac{1}{2} e^{-5it} + \\frac{i}{2} e^{-3it} - \\frac{i}{2} e^{3it} + \\frac{1}{2} e^{5it} \\square (b) Find the Fourier series for g(t) . What is the period of g(t) ? (The period of the product is more interesting. The product repeats every 2\u03c0 , that is, g(t+2\u03c0) = g(t) , so the period of g(t) is a divisor of 2\u03c0 . To determine the fundamental frequency of g(t) , we find its Fourier series.) Solution : \\begin{split} \\sin3t\u00b7\\cos5t &= \\frac{e^{3it} - e^{-3it}}{2i} \\cdot \\frac{e^{5it} + e^{-5it}}{2} \\\\ &=\\frac{1}{4i} (-e^{-8it} + e^{-2it} - e^{2it} + e^{8it}) \\end{split} From this Fourier series, we can see, \u03c0 is a period. \\sin 3(t+\u03c0) \\cdot \\cos 5(t+\u03c0) \\\\ = (- \\sin 3t) \\cdot (-\\cos 5(t)) \\\\ = \\sin3t\u00b7\\cos5t \\square","title":"1.3 Periods of sums and products"},{"location":"ch01ex/#14-different-definitions-of-periodicity","text":"(a) Show that f(t) is periodic of period p if and only if f(t\u2212p) = f(t) for all t . The upshot is that it doesn\u2019t matter if we define periodicity as f(t+p) = f(t) or as f(t\u2212p) = f(t) . Proof : If f(t) is periodic, then f(t+p) = f(t) , let s = t + p , then t = s - p . So we have f(s\u2212p) = f(s), \\forall s The other side is the same. \\square (b) Show that f(t) is periodic of period p if and only if f(t + p/2) = f(t\u2212p/2) for all t . Proof : If f(t) is periodic, then f(t+p) = f(t) , let s = t - p/2 , then t = s + p/2 . So we have f(s + p/2) = f(s\u2212p/2), \\forall s The other side is the same. \\square","title":"1.4 Di\ufb00erent definitions of periodicity"},{"location":"ch01ex/#15-overheard-at-a-problem-session","text":"Suppose two sinusoids have the same frequency but possibly di\ufb00erent amplitudes and phases. What about their sum? Each of the following answers was proposed at a problem session: (a) Has twice the frequency of the original sinusoids. (b) Is either zero or has exactly the same frequency as the original sinusoids. (c) May exhibit beats. (d) Is not necessarily periodic at all. Which is correct, and why? (Hint: Think in terms of complex exponentials.) Solution : This is covered at the end of Appendix B. \\square","title":"1.5. Overheard at a problem session ..."},{"location":"ch01ex/#16-low-voltage","text":"A periodic voltage is given by v(t) = 3 \\cos (2\u03c0\u03bd_1t\u22121.3)+5 \\cos (2\u03c0\u03bd_2t+0.5). Regardless of the frequencies \u03bd_1 , \u03bd_2 , the maximum voltage is always less than 8 , but it can be much smaller. Use MATLAB (or another program) to find the maximum voltage if \u03bd_1 = 2 Hz and \u03bd_2 = 1 Hz. Solution : Here is the python code t = np.arange(0, 2, 0.01) f = 3 * np.cos(2 * np.pi * 2 * t - 1.3) + 5 * np.cos(2 * np.pi * 1 * t + 0.5) plt.plot(t, f) plt.show() print(max(np.around(f, 4)), min(np.around(f, 4))) 5.7807 -7.6865","title":"1.6. Low voltage"},{"location":"ch01ex/#17-periodizing-a-triangle","text":"The triangle function with a parameter p > 0 is \\Lambda_p(t) = \\begin{cases} 1 - \\frac{1}{p}|t|, &|t| \\leq p, \\\\ 0, &|t| \\geq p. \\\\ \\end{cases} We\u2019ll be seeing it a lot. When p = 1 (the model case in many instances), we simplify the notation to just \u039b(t) . Note that \u039b_p(t) = \u039b(t/p) . The parameter p specifies the length of the base, namely 2p . Alternately, p determines the slopes of the sides: the left side has slope 1/p and the right side has slope \u22121/p . Now for T > 0 define g(t) = \\sum_{n = -\\infty}^{\\infty} \u039b_p(t - nT) Anwser: (a) For p = 1/2 and T = 1/2, T = 3/4, T = 1, T = 2 , sketch the graphs of g(t) . Periodic? In each case what is the period? Solution : The period are 0, 6/8, 1, 2 respectively. (b) In which of these cases is it possible to recover the original signal \u039b_{1/2}(t) from the sum g(t) ? What operation would you apply to g(t) to do this? Solution : When T = 1, 2 . We can times g(t) with the following h(t) h(t) = \\begin{cases} 1 &\\text{if } -1/2 \\leq t \\leq 1/2 \\\\ 0 &\\text{otherwise } \\\\ \\end{cases} \\square (c) In general, what condition on p and T will guarantee that \u039b_p(t) can be recovered from the sum g(t) ? Solution : In general, we need T \\geq 2p . \\square","title":"1.7. Periodizing a triangle"},{"location":"ch01notes/","text":"1. Fourier Series 1.1 Choices: Welcome Aboard The question of convergence of Fourier series, believe it or not, led G. Cantor near the turn of the 20th century to investigate and invent the theory of infinite sets and to distinguish di\ufb00erent cardinalities of infinite sets. 1.2 1.2.1. Time and space. The frequency and wavelength are related through the equation v = \u03bb\u03bd , which is the same as speed = distance/time. More on spatial periodicity. a crystal has a regular, repeating pattern of atoms in space; the arrangement of atoms is called a lattice. The function that describes the electron density distribution of the crystal is then a periodic function of the spatial variables, in three dimensions, that describes the crystal. 1.2.2. Definitions, examples, and things to come. f(t) is defined on \\mathbb{R} . The smallest T such that f(t+T) = f(t) is called the fundamental period of the function. Is the sum of two periodic functions periodic? No, consider \\cos t and \\cos \\sqrt[]{2}t . 1.2.3. The building blocks: A few more examples. The state of the harmonic oscillator system is described by a single sinusoid, say of the form A \\sin (2 \\pi \\nu t + \\phi ) The period is \\nu . When Fourier consider the problem when a ring is heated up, he came to the idea that the distribution of temperature can be modeled as a sum of sinusoids \\sum_{n=1}^{N} A_n \\sin (n \\theta + \\phi_n) 1.3 It All Adds Up It's a little bit awkward to use the above equation \\sum_{n=1}^{N} A_n \\sin (2 \\pi n t + \\phi_n) It\u2019s more common to write a general trigonometric sum as \\tag{1-3-1} \\frac{a_0}{2} + \\sum_{n = 1}^{N} a_n \\cos (2\u03c0nt) + b_n \\sin (2\u03c0nt) The above 2 equations are the same, because we have \\sin(\u03b1+\u03b2) = \\sin\u03b1\\cos\u03b2 +\\cos\u03b1\\sin\u03b2 . \\sin (2 \\pi n t + \\phi_n) = \\\\ \\sin (2 \\pi n t) \\cos (\\phi_n) + \\cos (2 \\pi n t) \\sin (\\phi_n) And let a_n = \\sin (\\phi_n), b_n = \\cos (\\phi_n) , then we can see they are the same. Using complex exponentials. Now from Appendix B, we know \\cos (2 \\pi n t) = \\frac{ e^{2\u03c0int} + e^{-2\u03c0int} }{2}, \\quad \\sin (2 \\pi n t) = \\frac{ e^{2\u03c0int} - e^{-2\u03c0int} }{2i} Then we can rearrange and simplify the equation (1-3-1). \\sum_{n = -N}^{N} c_n e^{2\u03c0int} \\\\ c_0 = \\frac{a_0}{2} \\\\ c_n = a_n - i b_n, \\text{ for } n > 0,\\\\ c_n = a_n + i b_n, \\text{ for } n < 0.\\\\ Let f(t) = \\sum_{n = -N}^{N} c_n e^{2\u03c0int} Note, c_n can be complex numbers. If f(t) is real, then \\overline{f(t)} = \\sum_{n = -N}^{N} \\overline{c_n e^{2\u03c0int}} = \\sum_{n = -N}^{N} \\overline{c_n} e^{-2\u03c0int} = f(t) So we have \\overline{c_n} = c_{-n} On the other hand, if c_n satisfy \\overline{c_n} = c_{-n} , f(t) = \\sum_{n = -N}^{N} c_n e^{2\u03c0int} \\\\ = c_0 + \\sum_{n = 1}^{N} c_n e^{2\u03c0int} + c_{-n} e^{-2\u03c0int} \\\\ = c_0 + \\sum_{n = 1}^{N} 2\\Re(c_n e^{2\u03c0int}) \\\\ = c_0 + \\Re\\left\\{ \\sum_{n = 1}^{N} c_n e^{2\u03c0int} \\right\\} \\square 1.3.1. Lost at c. Let f(t) = \\sum_{n = -N}^{N} c_n e^{2\u03c0int} and we want to solve c_k . Rearrange it a little bit c_k = e^{-2\u03c0ikt} f(t) - e^{-2\u03c0ikt} \\sum_{n \\neq k} c_n e^{2\u03c0int} \\\\ = e^{-2\u03c0ikt} f(t) - \\sum_{n \\neq k} c_n e^{2\u03c0i(n-k)t} This is as far as algebra will take us, and when algebra is exhausted, the desperate mathematician will turn to calculus, i.e., to di\ufb00erentiation or integration. Here\u2019s a hint: di\ufb00erentiation won\u2019t get you anywhere. Another idea is needed, and that idea is integrating both sides from 0 to 1. Note \\int_{0}^{1} e^{2\u03c0i(n-k)t} dt = \\\\ \\frac{1}{2\u03c0i(n-k)} e^{2\u03c0i(n-k)t} \\bigg|_{0}^{1} = \\\\ \\frac{1}{2\u03c0i(n-k)} (e^{2\u03c0i(n-k)} - 1) = 0 So c_k = \\int_{0}^{1} e^{-2\u03c0ikt} f(t) dt Let\u2019s summarize and be careful to note what we\u2019ve done here, and what we haven\u2019t done. We\u2019ve shown that if we can write a periodic function f(t) of period 1 as a sum f(t) = \\sum_{n = -N}^{N} c_n e^{2\u03c0int} then the coe\ufb03cients c_n must be given by c_n = \\int_{0}^{1} e^{-2\u03c0int} f(t) dt. We have not shown that every periodic function can be expressed this way. If we assume f(t) is real, then \\overline{c_n} = \\overline{\\int_{0}^{1} e^{-2\u03c0int} f(t) dt} = \\int_{0}^{1} \\overline{e^{-2\u03c0int} f(t) dt} = \\\\ \\int_{0}^{1} \\overline{e^{-2\u03c0int}} f(t) dt = \\\\ \\int_{0}^{1} e^{2\u03c0int} f(t) dt = c_{-n} \\square 1.3.2. Fourier coe\ufb03cients. This sum is called Fourier Series \\sum_{n = -N}^{N} c_n e^{2\u03c0int} Fourier coefficients, hip version \\hat{f}(n) = \\int_{0}^{1} e^{-2\u03c0int} f(t) dt The next step is to show given any a , \\hat{f}(n) = \\int_{a}^{a+1} e^{-2\u03c0int} f(t) dt In \"Understanding Analysis\", we learned that the 2nd part of The Fundamental Theorem of Calculus, if g(x) is integrable, and G(x) = \\int_{a}^{x} g(x) . If g(x) is continuous at x , then G'(x) = g(x) . Then let G(a) = \\int_{0}^{a} g(x) \\\\ H(a) = \\int_{0}^{a+1} g(x) Then G'(a) = g(a), H'(a) = G'(a+1) = g(a+1) So \\frac{d}{da} \\int_{a}^{a+1} g(x) = g(a+1) - g(a) . We assume this is also true for the complex function. Then \\frac{d}{da} \\hat{f}(n) = e^{-2\u03c0int} f(t) \\bigg|_{a}^{a+1} \\\\ = e^{-2\u03c0in(a+1)} f(a+1) - e^{-2\u03c0ina} f(a) \\\\ = e^{-2\u03c0in} e^{-2\u03c0ina} f(a+1) - e^{-2\u03c0ina} f(a) \\\\ = e^{-2\u03c0ina} f(a+1) - e^{-2\u03c0ina} f(a) \\\\ = 0 \\\\ That means \\hat{f}(n) is independent of a . There are times when the following is helpful \u2014 integrating over a symmetric interval, that is. \\int_{-1/2}^{1/2} e^{-2\u03c0int} f(t)dt Symmetry relations. If f(t) is real, we have seen \\overline{c_n} = c_{-n} \\Leftrightarrow \\overline{\\hat{f}(n)} = \\hat{f}(-n) \\\\ \\Rightarrow \\\\ |\\overline{\\hat{f}(n)}| = |\\hat{f}(-n)| So the magnitude of the Fourier coe\ufb03cients is an even function of n . Now assume f(t) is even function We have \\begin{split} \\hat{f}(-n) &= \\int_{0}^{1} e^{-2\u03c0i(-n)t} f(t) dt \\\\ &= \\int_{0}^{1} e^{-2\u03c0i(-n)t} f(-t) dt \\end{split} Now let t = g(s) = -s and recall the change-of-variable formula \\int_{a}^{b} f(g(x)) g'(x) dx = \\int_{g(a)}^{g(b)} f(t) dt We have \\int_{0}^{1} e^{-2\u03c0i(-n)t} f(t) dt = \\int_{0}^{-1} e^{-2\u03c0i(-n)(-s)} f(-s) g'(s) ds\\\\ = -\\int_{0}^{-1} e^{-2\u03c0ins} f(s) ds\\\\ = \\int_{-1}^{0} e^{-2\u03c0ins} f(s) ds\\\\ = \\hat{f}(n) So we have \\hat{f}(-n) = \\hat{f}(n) \\square If f(t) is even, then the Fourier coe\ufb03cients \\hat{f}(n) are also even. If f(t) is real and even, then the Fourier coe\ufb03cients \\hat{f}(n) are also real and even. Now assume f(t) is odd function. We have \\begin{split} \\hat{f}(-n) &= \\int_{0}^{1} e^{-2\u03c0i(-n)t} f(t) dt \\\\ &= -\\int_{0}^{-1} e^{-2\u03c0i(-n)(-s)} f(-s) dt \\\\ &= \\int_{0}^{-1} e^{-2\u03c0ins} f(s) dt \\\\ &= -\\int_{-1}^{0} e^{-2\u03c0ins} f(s) dt \\\\ &= - \\hat{f}(n) \\end{split} If f(t) is odd, then the Fourier coe\ufb03cients \\hat{f}(n) are also odd. If f(t) is real and odd, then the Fourier coe\ufb03cients \\hat{f}(n) are odd and purely imaginary. If we think of \\hat{f}(n) = \\int_{0}^{1} e^{-2\u03c0int}f(t)dt as a transform of f to a new function \\hat{f}(n) . While f(t) is defined for a continuous variable t , the transformed function \\hat{f}(n) is defined on the integers. There are reasons for this that are much deeper than simply solving for the unknown coe\ufb03cients in a Fourier series. 1.3.3. Period, frequencies, and spectrum. We\u2019re assuming that f(t) is a real, periodic signal of period 1 with a representation as the series f(t) = \\sum_{n = -\\infty }^{\\infty} \\hat{f}(n) e^{2\u03c0int} Rather, being able to write f(t) as a Fourier series means that it is synthesized from many harmonics, many frequencies, positive and negative, perhaps an infinite number. The set of frequencies n that are present in a given periodic signal is the spectrum of the signal. For a real signal, \\overline{\\hat{f}(n)} = \\hat{f}(-n) . Then the coe\ufb03cients \\hat{f}(n) and \\hat{f}(-n) are either both zero or both nonzero. If the coe\ufb03cients are all zero from some point on, say \\hat{f}(n) = 0 \\text{ for } |n| > N , it\u2019s common to say that the signal has no spectrum from that point on. One also says in this case that the signal is bandlimited and that the bandwidth is 2N . |\\hat{f}(n)|^2 is said to be the energy of the (positive and negative) harmonic e^{\\pm 2\u03c0int} . The sequence of squared magnitudes |\\hat{f}(n)|^2 is called the energy spectrum or the power spectrum. Rayleigh\u2019s identity: \\int_{0}^{1} |f(t)|^2 dt = \\sum_{n = -\\infty }^{\\infty} |\\hat{f}(n)|^2 Viewing a signal: Why do musical instruments sound di\ufb00erent? why do two instruments sound di\ufb00erent even when they are playing the same note? It\u2019s because the note that an instrument produces is not a single sinusoid of a single frequency, not a pure A at 440 Hz, for example, but a sum of many sinusoids each of its own frequency and each contributing its own amount of energy. two instruments sound di\ufb00erent because of the di\ufb00erent harmonics they produce and because of the di\ufb00erent strengths of the harmonics. To oversimplify, your inner ear (the cochlea) finds the harmonics and their amounts (the key is resonance) and passes that data on to your brain to do the synthesis. Nature knew Fourier analysis all along! 1.3.4. Changing the period, and another reciprocal relationship. Suppose f(t) has period T . Then let g(t) = f(Tt) , g(t+1) = f(T(t+1)) = f(Tt + T) = f(Tt) = g(t) So g has period of 1 . g(t) = \\sum_{n = -\\infty}^{\\infty} c_n e^{2\u03c0int} Let s = Tt f(s) = g(t) = \\sum_{n = -\\infty}^{\\infty} c_n e^{2\u03c0int} = \\sum_{n = -\\infty}^{\\infty} c_n e^{2\u03c0ins/T} Also c_n = \\int_{0}^{1} e^{-2\u03c0int} g(t) dt \\\\ = \\frac{1}{T} \\int_{0}^{T} e^{-2\u03c0in s/T} f(s) ds Sometimes, it's useful to write c_n = \\frac{1}{T} \\int_{-T/2}^{T/2} e^{-2\u03c0in s/T} f(s) ds Or take the harmonics as \\frac{1}{\\sqrt[]{T}} e^{2\u03c0int/T} \\\\ c_n = \\frac{1}{\\sqrt[]{T}} \\int_{0}^{T} e^{-2\u03c0int/T} f(t) dt Time domain \u2013 frequency domain reciprocity. Given f(t) = \\sum_{n = -\\infty}^{\\infty} c_n e^{2\u03c0int/T} In the time domain the signal repeats after T seconds, while the points in the spectrum are 0, \u00b11/T, \u00b12/T, ... , which are spaced 1/T apart. This is worth elevating to an aphorism: The larger the period in time, the smaller the spacing in the spectrum. The smaller the period in time, the larger the spacing in the spectrum.","title":"Chapter 01 Notes"},{"location":"ch01notes/#1-fourier-series","text":"","title":"1. Fourier Series"},{"location":"ch01notes/#11-choices-welcome-aboard","text":"The question of convergence of Fourier series, believe it or not, led G. Cantor near the turn of the 20th century to investigate and invent the theory of infinite sets and to distinguish di\ufb00erent cardinalities of infinite sets.","title":"1.1 Choices: Welcome Aboard"},{"location":"ch01notes/#12","text":"","title":"1.2"},{"location":"ch01notes/#121-time-and-space","text":"The frequency and wavelength are related through the equation v = \u03bb\u03bd , which is the same as speed = distance/time. More on spatial periodicity. a crystal has a regular, repeating pattern of atoms in space; the arrangement of atoms is called a lattice. The function that describes the electron density distribution of the crystal is then a periodic function of the spatial variables, in three dimensions, that describes the crystal.","title":"1.2.1. Time and space."},{"location":"ch01notes/#122-definitions-examples-and-things-to-come","text":"f(t) is defined on \\mathbb{R} . The smallest T such that f(t+T) = f(t) is called the fundamental period of the function. Is the sum of two periodic functions periodic? No, consider \\cos t and \\cos \\sqrt[]{2}t .","title":"1.2.2. Definitions, examples, and things to come."},{"location":"ch01notes/#123-the-building-blocks-a-few-more-examples","text":"The state of the harmonic oscillator system is described by a single sinusoid, say of the form A \\sin (2 \\pi \\nu t + \\phi ) The period is \\nu . When Fourier consider the problem when a ring is heated up, he came to the idea that the distribution of temperature can be modeled as a sum of sinusoids \\sum_{n=1}^{N} A_n \\sin (n \\theta + \\phi_n)","title":"1.2.3. The building blocks: A few more examples."},{"location":"ch01notes/#13-it-all-adds-up","text":"It's a little bit awkward to use the above equation \\sum_{n=1}^{N} A_n \\sin (2 \\pi n t + \\phi_n) It\u2019s more common to write a general trigonometric sum as \\tag{1-3-1} \\frac{a_0}{2} + \\sum_{n = 1}^{N} a_n \\cos (2\u03c0nt) + b_n \\sin (2\u03c0nt) The above 2 equations are the same, because we have \\sin(\u03b1+\u03b2) = \\sin\u03b1\\cos\u03b2 +\\cos\u03b1\\sin\u03b2 . \\sin (2 \\pi n t + \\phi_n) = \\\\ \\sin (2 \\pi n t) \\cos (\\phi_n) + \\cos (2 \\pi n t) \\sin (\\phi_n) And let a_n = \\sin (\\phi_n), b_n = \\cos (\\phi_n) , then we can see they are the same.","title":"1.3 It All Adds Up"},{"location":"ch01notes/#using-complex-exponentials","text":"Now from Appendix B, we know \\cos (2 \\pi n t) = \\frac{ e^{2\u03c0int} + e^{-2\u03c0int} }{2}, \\quad \\sin (2 \\pi n t) = \\frac{ e^{2\u03c0int} - e^{-2\u03c0int} }{2i} Then we can rearrange and simplify the equation (1-3-1). \\sum_{n = -N}^{N} c_n e^{2\u03c0int} \\\\ c_0 = \\frac{a_0}{2} \\\\ c_n = a_n - i b_n, \\text{ for } n > 0,\\\\ c_n = a_n + i b_n, \\text{ for } n < 0.\\\\ Let f(t) = \\sum_{n = -N}^{N} c_n e^{2\u03c0int} Note, c_n can be complex numbers. If f(t) is real, then \\overline{f(t)} = \\sum_{n = -N}^{N} \\overline{c_n e^{2\u03c0int}} = \\sum_{n = -N}^{N} \\overline{c_n} e^{-2\u03c0int} = f(t) So we have \\overline{c_n} = c_{-n} On the other hand, if c_n satisfy \\overline{c_n} = c_{-n} , f(t) = \\sum_{n = -N}^{N} c_n e^{2\u03c0int} \\\\ = c_0 + \\sum_{n = 1}^{N} c_n e^{2\u03c0int} + c_{-n} e^{-2\u03c0int} \\\\ = c_0 + \\sum_{n = 1}^{N} 2\\Re(c_n e^{2\u03c0int}) \\\\ = c_0 + \\Re\\left\\{ \\sum_{n = 1}^{N} c_n e^{2\u03c0int} \\right\\} \\square","title":"Using complex exponentials."},{"location":"ch01notes/#131-lost-at-c","text":"Let f(t) = \\sum_{n = -N}^{N} c_n e^{2\u03c0int} and we want to solve c_k . Rearrange it a little bit c_k = e^{-2\u03c0ikt} f(t) - e^{-2\u03c0ikt} \\sum_{n \\neq k} c_n e^{2\u03c0int} \\\\ = e^{-2\u03c0ikt} f(t) - \\sum_{n \\neq k} c_n e^{2\u03c0i(n-k)t} This is as far as algebra will take us, and when algebra is exhausted, the desperate mathematician will turn to calculus, i.e., to di\ufb00erentiation or integration. Here\u2019s a hint: di\ufb00erentiation won\u2019t get you anywhere. Another idea is needed, and that idea is integrating both sides from 0 to 1. Note \\int_{0}^{1} e^{2\u03c0i(n-k)t} dt = \\\\ \\frac{1}{2\u03c0i(n-k)} e^{2\u03c0i(n-k)t} \\bigg|_{0}^{1} = \\\\ \\frac{1}{2\u03c0i(n-k)} (e^{2\u03c0i(n-k)} - 1) = 0 So c_k = \\int_{0}^{1} e^{-2\u03c0ikt} f(t) dt Let\u2019s summarize and be careful to note what we\u2019ve done here, and what we haven\u2019t done. We\u2019ve shown that if we can write a periodic function f(t) of period 1 as a sum f(t) = \\sum_{n = -N}^{N} c_n e^{2\u03c0int} then the coe\ufb03cients c_n must be given by c_n = \\int_{0}^{1} e^{-2\u03c0int} f(t) dt. We have not shown that every periodic function can be expressed this way. If we assume f(t) is real, then \\overline{c_n} = \\overline{\\int_{0}^{1} e^{-2\u03c0int} f(t) dt} = \\int_{0}^{1} \\overline{e^{-2\u03c0int} f(t) dt} = \\\\ \\int_{0}^{1} \\overline{e^{-2\u03c0int}} f(t) dt = \\\\ \\int_{0}^{1} e^{2\u03c0int} f(t) dt = c_{-n} \\square","title":"1.3.1. Lost at c."},{"location":"ch01notes/#132-fourier-coefficients","text":"This sum is called Fourier Series \\sum_{n = -N}^{N} c_n e^{2\u03c0int} Fourier coefficients, hip version \\hat{f}(n) = \\int_{0}^{1} e^{-2\u03c0int} f(t) dt The next step is to show given any a , \\hat{f}(n) = \\int_{a}^{a+1} e^{-2\u03c0int} f(t) dt In \"Understanding Analysis\", we learned that the 2nd part of The Fundamental Theorem of Calculus, if g(x) is integrable, and G(x) = \\int_{a}^{x} g(x) . If g(x) is continuous at x , then G'(x) = g(x) . Then let G(a) = \\int_{0}^{a} g(x) \\\\ H(a) = \\int_{0}^{a+1} g(x) Then G'(a) = g(a), H'(a) = G'(a+1) = g(a+1) So \\frac{d}{da} \\int_{a}^{a+1} g(x) = g(a+1) - g(a) . We assume this is also true for the complex function. Then \\frac{d}{da} \\hat{f}(n) = e^{-2\u03c0int} f(t) \\bigg|_{a}^{a+1} \\\\ = e^{-2\u03c0in(a+1)} f(a+1) - e^{-2\u03c0ina} f(a) \\\\ = e^{-2\u03c0in} e^{-2\u03c0ina} f(a+1) - e^{-2\u03c0ina} f(a) \\\\ = e^{-2\u03c0ina} f(a+1) - e^{-2\u03c0ina} f(a) \\\\ = 0 \\\\ That means \\hat{f}(n) is independent of a . There are times when the following is helpful \u2014 integrating over a symmetric interval, that is. \\int_{-1/2}^{1/2} e^{-2\u03c0int} f(t)dt","title":"1.3.2. Fourier coe\ufb03cients."},{"location":"ch01notes/#symmetry-relations","text":"If f(t) is real, we have seen \\overline{c_n} = c_{-n} \\Leftrightarrow \\overline{\\hat{f}(n)} = \\hat{f}(-n) \\\\ \\Rightarrow \\\\ |\\overline{\\hat{f}(n)}| = |\\hat{f}(-n)| So the magnitude of the Fourier coe\ufb03cients is an even function of n . Now assume f(t) is even function We have \\begin{split} \\hat{f}(-n) &= \\int_{0}^{1} e^{-2\u03c0i(-n)t} f(t) dt \\\\ &= \\int_{0}^{1} e^{-2\u03c0i(-n)t} f(-t) dt \\end{split} Now let t = g(s) = -s and recall the change-of-variable formula \\int_{a}^{b} f(g(x)) g'(x) dx = \\int_{g(a)}^{g(b)} f(t) dt We have \\int_{0}^{1} e^{-2\u03c0i(-n)t} f(t) dt = \\int_{0}^{-1} e^{-2\u03c0i(-n)(-s)} f(-s) g'(s) ds\\\\ = -\\int_{0}^{-1} e^{-2\u03c0ins} f(s) ds\\\\ = \\int_{-1}^{0} e^{-2\u03c0ins} f(s) ds\\\\ = \\hat{f}(n) So we have \\hat{f}(-n) = \\hat{f}(n) \\square If f(t) is even, then the Fourier coe\ufb03cients \\hat{f}(n) are also even. If f(t) is real and even, then the Fourier coe\ufb03cients \\hat{f}(n) are also real and even. Now assume f(t) is odd function. We have \\begin{split} \\hat{f}(-n) &= \\int_{0}^{1} e^{-2\u03c0i(-n)t} f(t) dt \\\\ &= -\\int_{0}^{-1} e^{-2\u03c0i(-n)(-s)} f(-s) dt \\\\ &= \\int_{0}^{-1} e^{-2\u03c0ins} f(s) dt \\\\ &= -\\int_{-1}^{0} e^{-2\u03c0ins} f(s) dt \\\\ &= - \\hat{f}(n) \\end{split} If f(t) is odd, then the Fourier coe\ufb03cients \\hat{f}(n) are also odd. If f(t) is real and odd, then the Fourier coe\ufb03cients \\hat{f}(n) are odd and purely imaginary. If we think of \\hat{f}(n) = \\int_{0}^{1} e^{-2\u03c0int}f(t)dt as a transform of f to a new function \\hat{f}(n) . While f(t) is defined for a continuous variable t , the transformed function \\hat{f}(n) is defined on the integers. There are reasons for this that are much deeper than simply solving for the unknown coe\ufb03cients in a Fourier series.","title":"Symmetry relations."},{"location":"ch01notes/#133-period-frequencies-and-spectrum","text":"We\u2019re assuming that f(t) is a real, periodic signal of period 1 with a representation as the series f(t) = \\sum_{n = -\\infty }^{\\infty} \\hat{f}(n) e^{2\u03c0int} Rather, being able to write f(t) as a Fourier series means that it is synthesized from many harmonics, many frequencies, positive and negative, perhaps an infinite number. The set of frequencies n that are present in a given periodic signal is the spectrum of the signal. For a real signal, \\overline{\\hat{f}(n)} = \\hat{f}(-n) . Then the coe\ufb03cients \\hat{f}(n) and \\hat{f}(-n) are either both zero or both nonzero. If the coe\ufb03cients are all zero from some point on, say \\hat{f}(n) = 0 \\text{ for } |n| > N , it\u2019s common to say that the signal has no spectrum from that point on. One also says in this case that the signal is bandlimited and that the bandwidth is 2N . |\\hat{f}(n)|^2 is said to be the energy of the (positive and negative) harmonic e^{\\pm 2\u03c0int} . The sequence of squared magnitudes |\\hat{f}(n)|^2 is called the energy spectrum or the power spectrum. Rayleigh\u2019s identity: \\int_{0}^{1} |f(t)|^2 dt = \\sum_{n = -\\infty }^{\\infty} |\\hat{f}(n)|^2","title":"1.3.3. Period, frequencies, and spectrum."},{"location":"ch01notes/#viewing-a-signal-why-do-musical-instruments-sound-different","text":"why do two instruments sound di\ufb00erent even when they are playing the same note? It\u2019s because the note that an instrument produces is not a single sinusoid of a single frequency, not a pure A at 440 Hz, for example, but a sum of many sinusoids each of its own frequency and each contributing its own amount of energy. two instruments sound di\ufb00erent because of the di\ufb00erent harmonics they produce and because of the di\ufb00erent strengths of the harmonics. To oversimplify, your inner ear (the cochlea) finds the harmonics and their amounts (the key is resonance) and passes that data on to your brain to do the synthesis. Nature knew Fourier analysis all along!","title":"Viewing a signal: Why do musical instruments sound di\ufb00erent?"},{"location":"ch01notes/#134-changing-the-period-and-another-reciprocal-relationship","text":"Suppose f(t) has period T . Then let g(t) = f(Tt) , g(t+1) = f(T(t+1)) = f(Tt + T) = f(Tt) = g(t) So g has period of 1 . g(t) = \\sum_{n = -\\infty}^{\\infty} c_n e^{2\u03c0int} Let s = Tt f(s) = g(t) = \\sum_{n = -\\infty}^{\\infty} c_n e^{2\u03c0int} = \\sum_{n = -\\infty}^{\\infty} c_n e^{2\u03c0ins/T} Also c_n = \\int_{0}^{1} e^{-2\u03c0int} g(t) dt \\\\ = \\frac{1}{T} \\int_{0}^{T} e^{-2\u03c0in s/T} f(s) ds Sometimes, it's useful to write c_n = \\frac{1}{T} \\int_{-T/2}^{T/2} e^{-2\u03c0in s/T} f(s) ds Or take the harmonics as \\frac{1}{\\sqrt[]{T}} e^{2\u03c0int/T} \\\\ c_n = \\frac{1}{\\sqrt[]{T}} \\int_{0}^{T} e^{-2\u03c0int/T} f(t) dt","title":"1.3.4. Changing the period, and another reciprocal relationship."},{"location":"ch01notes/#time-domain-frequency-domain-reciprocity","text":"Given f(t) = \\sum_{n = -\\infty}^{\\infty} c_n e^{2\u03c0int/T} In the time domain the signal repeats after T seconds, while the points in the spectrum are 0, \u00b11/T, \u00b12/T, ... , which are spaced 1/T apart. This is worth elevating to an aphorism: The larger the period in time, the smaller the spacing in the spectrum. The smaller the period in time, the larger the spacing in the spectrum.","title":"Time domain \u2013 frequency domain reciprocity."},{"location":"unresolved/","text":"1. Fourier Series 1.2. Adding periodic functions I cannot prove the minimum period.","title":"1. Fourier Series"},{"location":"unresolved/#1-fourier-series","text":"","title":"1. Fourier Series"},{"location":"unresolved/#12-adding-periodic-functions","text":"I cannot prove the minimum period.","title":"1.2. Adding periodic functions"}]}